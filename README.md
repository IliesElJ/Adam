# Adam
Implementation of the Adam Optimizer for a feed-forward NN.

**Introduction and Data**
This project was implemented in the context of our Advanced ML class. We thank our teacher A. Stromme for his great and intuitive teaching throughout this semester.
The code in this repository can be ran with in the notebooks. We use synthetic data and the MSINT dataset that we have imported through *tensorflow*, but this is explained below.

**Notebooks**
The *hyperparams* notebook contains what is described in the parts 6. and 4.2 of our report meaning the first results compared the classic optimization and the hyperparameter exploration done with a 5-fold CV.
The *comparative* notebook contains what is described in part 5. comparing AdaGrad, Adadelta, RMSProp and Adam.
The *synthetic* notebook implements the Preliminary analysis (part 3.) exploring the performances of Adam on synthetically-generated multi-model datasets. 

Thank you.

