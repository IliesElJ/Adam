# Implementation of the Adam Optimizer for a Feed-Forward Neural Network

## Introduction and Data
This project was implemented in the context of our Advanced Machine Learning class. We extend our gratitude to our teacher, A. Stromme, for his excellent and intuitive teaching throughout this semester. The code in this repository can be run within the provided notebooks. For our analysis, we utilized synthetic data and the MSINT dataset, imported through TensorFlow. Details on the datasets and their usage are in the notebooks.

## Notebooks
- *hyperparams*: Contains the analysis described in sections 6 and 4.2 of our report. This includes initial results comparing the classical optimization approach with the Adam optimizer and hyperparameter exploration using 5-fold cross-validation.
- *comparative*: Aligns with section 5 of our report, providing a comparative study of AdaGrad, Adadelta, RMSProp, and Adam.
- *preliminary*: Implements the preliminary analysis (part 3) of the report, exploring Adam's performance on synthetically-generated multi-modal datasets.

_This README is part of our project documentation for the Advanced ML class._
